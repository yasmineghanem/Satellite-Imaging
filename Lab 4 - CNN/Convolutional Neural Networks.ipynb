{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students Information\n",
    "\n",
    "Please enter the names and IDs of the two students below:\n",
    "\n",
    "1. **Name**: Yasmine Ghanem  \n",
    "   **ID**: `9203707` \n",
    "\n",
    "2. **Name**: Yasmin Elgendi  \n",
    "   **ID**: `9203717` \n",
    "\n",
    "3. **Name**: Sarah Mohamed   \n",
    "   **ID**: `9XXXXXX` \n",
    "\n",
    "4. **Name**: Basma Elhoseny  \n",
    "   **ID**: `9XXXXXX` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students Instructions\n",
    "\n",
    "This is your fourth graded lab assignment, as you put the work you have studied in the lectures in action, please take this opportunity to enhance your understanding of the concepts and hone your skills. As you work on your assignment, please keep the following instructions in mind:\n",
    "\n",
    "- Clearly state your personal information where indicated.\n",
    "- Be ready with your work before the time of the next discussion slot in the schedule.\n",
    "- Plagiarism will be met with penalties, refrain from copying any answers to make the most out of the assignment. If any signs of plagiarism are detected, actions will be taken.\n",
    "- It is acceptable to share the workload of the assignment bearing the discussion in mind.\n",
    "- Feel free to [reach out](mailto:cmpsy27@gmail.com) if there were any ambiguities or post on the classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To ensure a smooth evaluation process, please follow these steps for submitting your work:\n",
    "\n",
    "1. **Prepare Your Submission:** Alongside your main notebook, include any additional files that are necessary for running the notebook successfully. This might include data files, images, or supplementary scripts.\n",
    "\n",
    "2. **Rename Your Files:** Before submission, please rename your notebook to reflect the IDs of the two students working on this project. The format should be `ID1_ID2`, where `ID1` and `ID2` are the student IDs. For example, if the student IDs are `9123456` and `9876543`, then your notebook should be named `9123456_9876543.ipynb`.\n",
    "\n",
    "3. **Check for Completeness:** Ensure that all required tasks are completed and that the notebook runs from start to finish without errors. This step is crucial for a smooth evaluation.\n",
    "\n",
    "4. **Submit Your Work:** Once everything is in order, submit your notebook and any additional files via the designated submission link on Google Classroom **(code: 2yj6e24)**. Make sure you meet the submission deadline to avoid any late penalties.\n",
    "5. Please, note that the same student should submit the assignments for the pair throughout the semester.\n",
    "\n",
    "By following these instructions carefully, you help us in evaluating your work efficiently and fairly **and any failure to adhere to these guidelines can affect your grades**. If you encounter any difficulties or have questions about the submission process, please reach out as soon as possible.\n",
    "\n",
    "We look forward to seeing your completed projects and wish you the best of luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Instructions\n",
    "\n",
    "In this lab assignment, we require additional Python libraries for machine learning (ML) and deep learning (DL) algorithms and frameworks. To fulfill these requirements, we need to install Pytorch. \n",
    "1. Install Pytorch \\\n",
    "PyTorch is a versatile and powerful machine learning library for Python, known for its flexibility and ease of use in research and production. It supports various deep learning operations and models, including convolutional and recurrent neural networks. For Windows users, the installation also requires ensuring that CUDA, provided by NVIDIA, is compatible to enable GPU acceleration. This enhances performance significantly, particularly in training large neural networks.\\\n",
    "For windows installation with GPU support you can [check out this link](https://pytorch.org/get-started/locally/) which is the source for the command below and please know that support for GPU is done for windows so you can also check out [previous versions](https://pytorch.org/get-started/previous-versions/), you could use CPU on windows smoothly, use linux or resort to [WSL](https://www.youtube.com/watch?v=R4m8YEixidI).\n",
    "\n",
    "```bash\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** You are allowed to install any other necessary libraries you deem useful for solving the lab. Please ensure that any additional libraries are compatible with the project requirements and are properly documented in your submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "Machine learning is a field of artificial intelligence that enables systems to learn from data and make decisions without being explicitly programmed. It involves algorithms that iteratively learn from data and improve their accuracy over time by optimizing an error metric, typically through the use of loss functions. These functions quantify the difference between the predicted outputs and the actual outputs, guiding the algorithm to minimize this error during training. By continually adjusting and improving, machine learning models can achieve remarkable accuracy in tasks ranging from simple classification to complex decision-making scenarios.\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a specialized kind of neural networks particularly effective for analyzing visual imagery. They employ a mathematical operation known as convolution, which uses sliding window techniques to process data. This method is highly efficient in recognizing patterns and features in images due to the sparse interactions and parameter sharing of convolutional layers. These characteristics allow CNNs to capture local patterns like edges and textures with a significantly reduced amount of parameters compared to fully connected networks, making them highly efficient for tasks involving high-dimensional data like images.\n",
    "\n",
    "### Key Components of CNN Architecture\n",
    "\n",
    "The architecture of a Convolutional Neural Network (CNN) typically includes several key components, each playing a crucial role in the network's ability to process and interpret visual information:\n",
    "\n",
    "- **Convolutional Layers**: \n",
    "  - Apply a number of filters to the input.\n",
    "  - Create feature maps that capture essential details within the data.\n",
    "\n",
    "- **Pooling Layers**: \n",
    "  - Often follow convolutional layers.\n",
    "  - Reduce the spatial size of the feature maps, decreasing the computational load.\n",
    "  - Extract dominant features that are invariant to small changes in the input.\n",
    "\n",
    "- **Fully Connected Layers**: \n",
    "  - Interpret the features extracted by convolutional and pooling layers.\n",
    "  - Perform classification or regression tasks based on the interpreted features.\n",
    "\n",
    "- **Softmax Layer**:\n",
    "  - Typically used in the final layer if the task is classification.\n",
    "  - Outputs the probabilities of the instance belonging to each class.\n",
    "\n",
    "- **Dropout Layers**:\n",
    "  - Included to prevent overfitting.\n",
    "  - Randomly drop units from the neural network during the training process to improve generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of CNN Architecture: LeNet-5\n",
    "\n",
    "![Example: LeNet-5 Architecture](lenet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Neural Network Hyperparameters\n",
    "\n",
    "In training neural networks, various hyperparameters must be predefined to guide the training process. These include:\n",
    "\n",
    "- **Batch Size**\n",
    "  - **Definition**: Number of training examples used per iteration.\n",
    "  - **Common Values**: 32, 64, 128, 256.\n",
    "\n",
    "- **Learning Rate**\n",
    "  - **Definition**: Step size at each iteration to minimize the loss function.\n",
    "  - **Common Values**: Often starts small, e.g., 0.01, 0.001, and can be dynamically adjusted (lowered as we get nearer to minima, check scheduers).\n",
    "\n",
    "- **Optimizer Type**\n",
    "  - **Examples**: SGD (Stochastic Gradient Descent), Adam, RMSprop, Adagrad.\n",
    "  - **Role**: Different optimizers affect training dynamics and model performance differently.\n",
    "\n",
    "- **Beta Parameters for Adam Optimizer**\n",
    "  - **Beta1 and Beta2**: Control the decay rates of moving averages of past gradients and squared gradients.\n",
    "  - **Common Values**: Beta1 = 0.9, Beta2 = 0.999.\n",
    "\n",
    "- **Epochs**\n",
    "  - **Definition**: One full pass of the training dataset through the learning algorithm.\n",
    "  - **Usage**: More epochs can lead to better learning but risk overfitting.\n",
    "\n",
    "- **Momentum**\n",
    "  - **Definition**: Helps accelerate SGD and dampens oscillations.\n",
    "  - **Common Values**: 0.9, 0.99.\n",
    "\n",
    "- **Regularization Parameters (L1 & L2)**\n",
    "  - **Purpose**: Prevent overfitting by adding a penalty on the size of the coefficients.\n",
    "  - **Parameters**: Lambda or alpha values for L1 (Lasso) and L2 (Ridge) regularization.\n",
    "\n",
    "- **Dropout Rate**\n",
    "  - **Definition**: Fraction of neurons to randomly drop during training.\n",
    "  - **Common Values**: 0.2, 0.3, 0.5.\n",
    "\n",
    "- **Number of Layers and Number of Neurons in Each Layer**\n",
    "  - **Role**: Determines the architecture's depth and width, affecting its ability to capture complex patterns.\n",
    "\n",
    "- **Error Function Type for Classification with Softmax**\n",
    "  - **Definition**: Cross-entropy loss, also known as log loss.\n",
    "  - **Purpose**: Measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label (an alternative can be Categorical Hinge Loss).\n",
    "\n",
    "- **Early Stopping**\n",
    "  - **Purpose**: Halt training when performance on a validation set starts to worsen to prevent overfitting.\n",
    "\n",
    "Each of these hyperparameters can significantly impact the effectiveness and efficiency of the neural network training process, and tuning them appropriately is crucial for achieving optimal performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Req- Image Classification for EuroSATallBands\n",
    "This is the same problem of the previous lab but we will explore the power of machine learning. Image classification in remote sensing is not as popular as pixel classification but we will get to that later. \n",
    "\n",
    "- **Load the Images**: Load the images of the EuroSAT dataset that belong to the **residential**, **river**, and **forest** classes.\n",
    "\n",
    "- **Split the Dataset**: Split the dataset such that 10% of each class is used as validation data and other 10% is used as testing data, and the remainder is used for training your classifier. Use the indices provided by `np.random.choice` with seed set to `27`. **Code is provided do not change it**.\n",
    "\n",
    "- **CNN Architecture**: Use or implement one of the popular suitable CNN architectures, even with pretrained weights if you like.\n",
    "\n",
    "- **Hyperparameters Tuning**: According to your validation accuaracy, you should make altercations to your architectures and hyperparameters, you can even change the architecture altogether, the loss function, optimizer or others.\n",
    " \n",
    "- **Report Accuracy and Average F1 Score**: After testing your classifier on the test set, report the **Accuracy** and **Average F1 Score** of your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your libraries here\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage import io\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "## Training set indices.\n",
    "np.random.seed(27)  # Set random seed for reproducibility\n",
    "\n",
    "# Randomly select indices for the test sets for each class\n",
    "residential_indices = np.random.choice(np.arange(3000), size=600, replace=False)\n",
    "forest_indices = np.random.choice(np.arange(3000), size=600, replace=False)\n",
    "river_indices = np.random.choice(np.arange(2500), size=500, replace=False)\n",
    "\n",
    "residential_val_indices = residential_indices[:300]\n",
    "forest_val_indices = forest_indices[:300]\n",
    "river_val_indices = river_indices[:250]\n",
    "\n",
    "residential_test_indices = residential_indices[300:]\n",
    "forest_test_indices = forest_indices[300:]\n",
    "river_test_indices = river_indices[250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices for the training set\n",
    "all_indices = np.arange(3000)\n",
    "residential_training_indices = np.setdiff1d(all_indices, residential_indices)\n",
    "forest_training_indices = np.setdiff1d(all_indices, forest_indices)\n",
    "all_indices = np.arange(2500)\n",
    "river_training_indices = np.setdiff1d(all_indices, river_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Read the dataset\n",
    "def read_images(data_dir, indices, label):\n",
    "    images = []\n",
    "    for i in indices:\n",
    "        final_path = os.path.join(data_dir, label + '_' + str(i+1) + '.tif')\n",
    "        image = io.imread(final_path)\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "# Read the training images\n",
    "residential_training_images = read_images('../dataset/tif/Residential', residential_training_indices, 'Residential')\n",
    "forest_training_images = read_images('../dataset/tif/Forest', forest_training_indices, 'Forest')\n",
    "river_training_images = read_images('../dataset/tif/River', river_training_indices, 'River')\n",
    "\n",
    "# Read the validation images\n",
    "residential_val_images = read_images('../dataset/tif/Residential', residential_val_indices, 'Residential')\n",
    "forest_val_images = read_images('../dataset/tif/Forest', forest_val_indices, 'Forest')\n",
    "river_val_images = read_images('../dataset/tif/River', river_val_indices, 'River')\n",
    "\n",
    "# Read the test images\n",
    "residential_test_images = read_images('../dataset/tif/Residential', residential_test_indices, 'Residential')\n",
    "forest_test_images = read_images('../dataset/tif/Forest', forest_test_indices, 'Forest')\n",
    "river_test_images = read_images('../dataset/tif/River', river_test_indices, 'River')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 13, 64, 64)\n",
      "(2400, 13, 64, 64)\n",
      "(2000, 13, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the input images\n",
    "print(residential_training_images.shape) \n",
    "print(forest_training_images.shape) \n",
    "print(river_training_images.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(images, labels):\n",
    "    # shuffle the dataset\n",
    "     \n",
    "    # zip the images and labels together\n",
    "    zipped_lists = list(zip(images, labels))\n",
    "\n",
    "    # shuffle the zipped list\n",
    "    np.random.shuffle(zipped_lists)\n",
    "\n",
    "    # unzip the zipped list\n",
    "    shuflled_images, shuffled_labels = zip(*zipped_lists)\n",
    "\n",
    "    return shuflled_images, shuffled_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# CNN Model\n",
    "# Define LeNet-5 architecture\n",
    "def build_lenet(input_shape):\n",
    "  # Define Sequential Model\n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  # C1 Convolution Layer\n",
    "  model.add(tf.keras.layers.Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='tanh', input_shape=input_shape))\n",
    "  \n",
    "  # S2 SubSampling Layer\n",
    "  model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "  # C3 Convolution Layer\n",
    "  model.add(tf.keras.layers.Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='tanh'))\n",
    "\n",
    "  # S4 SubSampling Layer\n",
    "  model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "  # C5 Fully Connected Layer\n",
    "  model.add(tf.keras.layers.Dense(units=120, activation='tanh'))\n",
    "\n",
    "  # Flatten the output so that we can connect it with the fully connected layers by converting it into a 1D Array\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "  # FC6 Fully Connected Layers\n",
    "  model.add(tf.keras.layers.Dense(units=84, activation='tanh'))\n",
    "\n",
    "  # Output Layer\n",
    "  model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "  # Compile the Model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer= tf.keras.optimizers.legacy.SGD(lr=0.1, momentum=0.0, decay=0.0), metrics=['accuracy'])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - 5s 78ms/step - loss: 1.0963 - accuracy: 0.3513\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 4s 80ms/step - loss: 1.0957 - accuracy: 0.3529\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 4s 80ms/step - loss: 1.0961 - accuracy: 0.3534\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 4s 79ms/step - loss: 1.0961 - accuracy: 0.3494\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 4s 74ms/step - loss: 1.0956 - accuracy: 0.3488\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 4s 78ms/step - loss: 1.0957 - accuracy: 0.3516\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 4s 77ms/step - loss: 1.0961 - accuracy: 0.3472\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 4s 77ms/step - loss: 1.0956 - accuracy: 0.3525\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 4s 72ms/step - loss: 1.0956 - accuracy: 0.3506\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 4s 70ms/step - loss: 1.0954 - accuracy: 0.3566\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.0968 - accuracy: 0.3529\n",
      "Accuracy :  0.3529411852359772\n",
      "Training Data (6800, 64, 64, 13) (6800, 3)\n",
      "Test Data (850, 64, 64, 13) (850, 3)\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate([residential_training_images, forest_training_images, river_training_images], axis=0)\n",
    "y_train = np.concatenate([np.zeros(len(residential_training_images)), \n",
    "                                np.ones(len(forest_training_images)), \n",
    "                                np.ones(len(river_training_images)) * 2])\n",
    "\n",
    "x_test = np.concatenate([residential_test_images, forest_test_images, river_test_images], axis=0)\n",
    "y_test = np.concatenate([np.zeros(len(residential_test_images)), \n",
    "                               np.ones(len(forest_test_images)), \n",
    "                               np.ones(len(river_test_images)) * 2])\n",
    "\n",
    "rows, cols, bands = 64, 64, 13\n",
    "num_classes = 3  \n",
    "\n",
    "# Reshape the data into a 4D Array\n",
    "x_train = x_train.reshape(x_train.shape[0], rows, cols, bands)\n",
    "x_test = x_test.reshape(x_test.shape[0], rows, cols, bands)\n",
    "\n",
    "input_shape = (rows, cols, bands)\n",
    "\n",
    "# Set type as float32 and normalize the values to [0,1]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# Transform labels to one hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "lenet = build_lenet(input_shape)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "history = lenet.fit(x_train, y_train, epochs=epochs, batch_size=128, verbose=1)\n",
    "\n",
    "# Check Accuracy of the Model\n",
    "loss, acc = lenet.evaluate(x_test, y_test)\n",
    "print('Accuracy : ', acc)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], rows, cols, bands)\n",
    "print('Training Data', x_train.shape, y_train.shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], rows, cols, bands)\n",
    "print('Test Data', x_test.shape, y_test.shape)\n",
    "\n",
    "# Make Prediction\n",
    "image_index = 0  # choose any index you want to predict\n",
    "pred = lenet.predict(x_test[image_index].reshape(1, rows, cols, bands))\n",
    "print(pred.argmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 9ms/step\n",
      "Confusion Matrix:\n",
      "[[300   0   0]\n",
      " [300   0   0]\n",
      " [250   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35294117647058826\n",
      "Average F1 Score: 0.17391304347826086\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert one-hot encoded labels to integer format\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Check Accuracy of the Model\n",
    "y_pred = lenet.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test_int, y_pred_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "ax = sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\", xticklabels=['Residential', 'Forest', 'River'], yticklabels=['Residential', 'Forest', 'River'])\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test_int, y_pred_classes)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test_int, y_pred_classes, average='macro')\n",
    "print(\"Average F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35294117647058826\n",
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define LeNet-5 architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=13, out_channels=6, kernel_size=5)\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=5)\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=6 * 13 * 13, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=3)\n",
    "        #softmax layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = self.avgpool1(x)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = x.view(-1, 6 * 13 * 13)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = nn.functional.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = LeNet()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.0, weight_decay=0.0)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Combine training and test data\n",
    "x_train = np.concatenate([residential_training_images, forest_training_images, river_training_images], axis=0)\n",
    "y_train = np.concatenate([np.zeros(len(residential_training_images)), \n",
    "                          np.ones(len(forest_training_images)), \n",
    "                          np.ones(len(river_training_images)) * 2])\n",
    "\n",
    "x_test = np.concatenate([residential_test_images, forest_test_images, river_test_images], axis=0)\n",
    "y_test = np.concatenate([np.zeros(len(residential_test_images)), \n",
    "                         np.ones(len(forest_test_images)), \n",
    "                         np.ones(len(river_test_images)) * 2])\n",
    "\n",
    "rows, cols, bands = 64, 64, 13\n",
    "num_classes = 3  \n",
    "\n",
    "# Reshape the data into a 4D Array\n",
    "x_train = x_train.reshape(x_train.shape[0], bands, rows, cols).astype(np.float32)\n",
    "x_test = x_test.reshape(x_test.shape[0], bands, rows, cols).astype(np.float32)\n",
    "\n",
    "# Normalize the data to [0,1]\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    total = y_test_tensor.size(0)\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy:', accuracy)\n",
    "\n",
    "# Make Prediction\n",
    "image_index = 0  # choose any index you want to predict\n",
    "input_image = x_test_tensor[image_index].unsqueeze(0)\n",
    "output = model(input_image)\n",
    "predicted_class = torch.argmax(output).item()\n",
    "print('Predicted Class:', predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - 5s 72ms/step - loss: 1.0972 - accuracy: 0.3472 - val_loss: 1.3774 - val_accuracy: 0.3529\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 1.0957 - accuracy: 0.3475 - val_loss: 1.3765 - val_accuracy: 0.3529\n",
      "Epoch 3/10\n",
      "21/54 [==========>...................] - ETA: 2s - loss: 1.0956 - accuracy: 0.3594"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     72\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mcategorical_crossentropy, optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, score[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Yasmin ElGendi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate([residential_training_images, forest_training_images, river_training_images], axis=0)\n",
    "y_train = np.concatenate([np.zeros(len(residential_training_images)), \n",
    "                                np.ones(len(forest_training_images)), \n",
    "                                np.ones(len(river_training_images)) * 2])\n",
    "\n",
    "x_val = np.concatenate([residential_val_images, forest_val_images, river_val_images], axis=0)\n",
    "y_val = np.concatenate([np.zeros(len(residential_val_images)), \n",
    "                                np.ones(len(forest_val_images)), \n",
    "                                np.ones(len(river_val_images)) * 2])\n",
    "\n",
    "x_test = np.concatenate([residential_test_images, forest_test_images, river_test_images], axis=0)\n",
    "y_test = np.concatenate([np.zeros(len(residential_test_images)), \n",
    "                               np.ones(len(forest_test_images)), \n",
    "                               np.ones(len(river_test_images)) * 2])\n",
    "\n",
    "rows, cols, bands = 64, 64, 13\n",
    "num_classes = 3  \n",
    "\n",
    "# Reshape the data into a 4D Array\n",
    "x_train = x_train.reshape(x_train.shape[0], rows, cols, bands)\n",
    "x_test = x_test.reshape(x_test.shape[0], rows, cols, bands)\n",
    "x_val = x_val.reshape(x_val.shape[0], rows, cols, bands)\n",
    "\n",
    "input_shape = (rows, cols, bands)\n",
    "\n",
    "# Set type as float32 and normalize the values to [0,1]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "x_val /= 255.0\n",
    "\n",
    "# Transform labels to one hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# Importing necessary libraries\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "# Building the Model Architecture\n",
    "model = Sequential()\n",
    "# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 66 feature maps. The size of each feature map is 325 + 1 = 28325 + 1 = 28.\n",
    "# That is, the number of neurons has been reduced from 10241024 to 28  28 = 784 28  28 = 784.\n",
    "# Parameters between input layer and C1 layer: 6  (5  5 + 1)\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(rows, cols, bands)))\n",
    "\n",
    "# The input of this layer is the output of the first layer, which is a 28 * 28 * 6 node matrix.\n",
    "# The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 14 * 14 * 6.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# The input matrix size of this layer is 14 * 14 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.\n",
    "# The output matrix size of this layer is 10 * 10 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "\n",
    "# The input matrix size of this layer is 10 * 10 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2, so the output matrix size of this layer is 5 * 5 * 16.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# The input matrix size of this layer is 5 * 5 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #\n",
    "# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.\n",
    "# The number of output nodes in this layer is 120, with a total of 5 * 5 * 16 * 120 + 120 = 48120 parameters.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "# The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)\n",
    "model.add(Dense(84, activation='relu'))\n",
    "# The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Luck from Psy and wish me well \n",
    "# I wish you you well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Rubric (Total: 10 Marks)\n",
    "\n",
    "The lab is graded based on the following criteria:\n",
    "\n",
    "1. **Data Loading and Preparation (2 Marks)**\n",
    "   - Correctly loads images for the residential, river, and forest classes. (1 Mark)\n",
    "   - Accurately splits the dataset into training, validation testing subsets and clearly shows this split. (1 Mark)\n",
    "\n",
    "2. **CNN Architecture (3 Marks)**\n",
    "   - Uses appropriate CNN Architecture to the problem with a full pipeline. (2 Marks)\n",
    "   - Justifies the selection of CNN architecture. (1 Mark)\n",
    "\n",
    "3. **Hyperparameters Tuning (2 Marks)**\n",
    "   - Report evaluation metrics on validation set. (1 Mark)\n",
    "   - Analyzes results and tunes hyperparameters. (1 Mark)\n",
    "\n",
    "4. **Model Evaluation and Understanding (3 Marks)**\n",
    "   - Shows **confusion matrix** and correctly calculates and clearly shows the calculations for Accuracy and Average F1 Score. (1 Mark)\n",
    "   - **Comparison amongst your peers.** Compares the model's performance against those of peers to identify strengths and areas for improvement. (2 Marks)\n",
    "\n",
    "Each section of the lab will be evaluated on completeness, and correctness in approach and analysis. Part of the rubric also includes the student's ability to explain and justify their choices and results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
